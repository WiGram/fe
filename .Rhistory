# Intermediate computations
diff        <- x - x_bar
diff_sqr    <- diff ** 2
diff_lag    <- lag(x = x, n = lags) - x_bar
# Subsetting to remove NA
diff        <- diff[-c(1:lags)]
diff_sqr    <- diff_sqr[-c(1:lags)]
diff_lag    <- diff_lag[-c(1:lags)]
# Final computation
acf         <- sum(diff * diff_lag) / sum(diff_sqr)
return(acf)
}
library(ggplot2)
library(gridExtra)
# Data generation
time_steps <- 1000
a <- c(0.1, 0.7, 1.1, 3.1)
z <- rnorm(n = length(a) * time_steps,
mean = 0,
sd = 1)
# Matrix generation
x <- matrix(0, length(a), time_steps)
z <- matrix(z, length(a), time_steps)
# Simulate time series using ARCH(1)
for (i in 2:length(x[1,])){
s <- sqrt(1 + a * x[, i-1] ** 2)
x[, i] <- s * z[, i]
}
# Generate graphs for each level of a (alpha)
g <- lapply(1:length(a), function(i) {
ggplot(NULL, aes(x = 1:time_steps,
y = x[i, ],
col = 'red')) +
geom_line() +
labs(title = paste0("alpha: ", a[i], sep = ""),
x = NULL,
y = "ARCH(1) Returns") +
guides(col = FALSE)
})
# Plotting.
grid.arrange(grobs = g, ncol = 2)
# ==================================== #
# ===== Normal distribution fct ====== #
# ==================================== #
sim_norm <- rnorm(1000, 0, 1)
xfit     <- seq(min(sim_norm),
max(sim_norm),
length = length(sim_norm))
yfit     <- dnorm(xfit,
mean = mean(sim_norm),
sd = sd(sim_norm))
ggplot(NULL, aes(sim_norm)) +
geom_histogram(col = 'white',
fill = 'black',
bins = 50,
aes(y = ..density..)) +
geom_line(aes(x = xfit,
y = yfit,
col = 'red')) +
guides(col = FALSE)
# Notice something does not work with cauchy
sim_cchy <- rcauchy(100, 0, 1)
xfit     <- seq(min(sim_cchy),
max(sim_cchy),
length = length(sim_cchy))
yfit     <- dcauchy(xfit, 0, 1)
ggplot(NULL, aes(sim_cchy)) +
geom_histogram(col = 'white',
fill = 'black',
bins = 25,
aes(y = ..density..)) +
geom_line(aes(x = xfit,
y = yfit,
col = 'red')) +
guides(col = FALSE)
# ==================================== #
# ===== Cauchy distribution fct ====== #
# ==================================== #
norm_pdf <- function(x, mean, vol){
1 / sqrt(2 * pi * vol ** 2) * exp(-((x - mean)/vol) ** 2 / 2)
}
cchy_pdf <- function(x){
1 / pi * 1 / (1 + x ** 2)
}
vector <- seq(-20,20,0.01)
npdf <- norm_pdf(vector, 0, 1)
cpdf <- cchy_pdf(vector)
pdfs <- ggplot(NULL, aes(x = vector)) +
geom_line(aes(y = npdf, col = 'normal')) +
geom_line(aes(y = cpdf, col = 'cauchy'))
ncdf <- cumsum(n * 0.01)
ccdf <- cumsum(c * 0.01)
cdfs <- ggplot(NULL, aes(x = vector)) +
geom_line(aes(y = ncdf, col = 'normal')) +
geom_line(aes(y = ccdf, col = 'cauchy'))
grid.arrange(pdfs, cdfs, ncol = 1)
ncdf <- cumsum(npdf * 0.01)
ccdf <- cumsum(cpdf * 0.01)
cdfs <- ggplot(NULL, aes(x = vector)) +
geom_line(aes(y = ncdf, col = 'normal')) +
geom_line(aes(y = ccdf, col = 'cauchy'))
grid.arrange(pdfs, cdfs, ncol = 1)
# ================================================= #
# ================= Exercise 2 ==================== #
# ================================================= #
# Simulate y = 0.5 + 0.8 * x + eps,
# where x ~ U(0,1) and eps ~ N(0,1)
# ===== Model parameters ===== #
sims <- 500000
mean <- 0
sd   <- 1
min  <- 0
max  <- 1
b0   <- 0.5
b1   <- 0.8
# First approach - ptm for starting time
# ============================================== #
ptm   <- proc.time()
eps   <- rnorm(n = sims, mean = mean,  sd = sd)
x     <- runif(n = sims, min = min,    max = max)
y     <- b0 + b1 * x + eps
reg1  <- lm(y ~ x)
time1 <- proc.time() - ptm
# ============================================== #
# second approach - again, start timing with ptm
# ============================================== #
ptm <- proc.time()
y2   <- rep(0, sims)
x2   <- rep(0, sims)
eps2 <- rep(0, sims)
for (i in 1:sims){
# i <- 1
eps2[i] <- rnorm(n = 1, mean = mean, sd = sd)
x2[i]   <- runif(n = 1, min = min,   max = max)
y2[i]   <- b0 + b1 * x2[i] + eps2[i]
}
reg2  <- lm(y2 ~ x2)
time2 <- proc.time() - ptm
# ============================================== #
# ===== Comparison ============================= #
paste0("Method 1: Time elapsed: ", round(time1[3], 4), sep = "")
paste0("Method 2: Time elapsed: ", round(time2[3], 4), sep = "")
# ============================================== #
# ============================================== #
# ===== Analyse coefficients from method 1 ===== #
# ============================================== #
# The coefficients are found in either
# method below:
X <- cbind(1, x)
# (1)
reg_a <- lm(y ~ x)
eps_a <- reg_a$residuals
var_a <- 1 / (sims - 1) * t(eps_a) %*% eps_a
cov_a <- var_a[1,1] * solve(t(X) %*% X)
# (2)
reg_b <- solve(t(X) %*% X) %*% (t(X) %*% y)
eps_b <- y - reg_b[1,] - reg_b[2,] * x
var_b <- 1 / (sims - 1) * t(eps_b) %*% eps_b
cov_b <- var_b[1,1] * solve(t(X) %*% X)
reg_a
reg_b
cov_a
cov_b
# ============================================== #
# ===== Analyse coefficients from method 2 ===== #
# ============================================== #
X2 <- cbind(1, x2)
eps_2 <- reg2$residuals
var_2 <- 1 / (sims - 1) * t(eps_2) %*% eps_2
cov_2 <- var_2[1,1] * solve(t(X2) %*% X2)
reg2
cov_2
# ============================================== #
#' Conclusion:
#' Whether done in a for loop or by vectorisation,
#' both methods seem to produce the same results.
#' Vectorisation is considerably faster, which
#' is especially fruitful when doing many simulations.
#'
#' Finally, the lm method (a) can be done manually (b)
#' to produce the exact same results.
# ================================================= #
# ================= Exercise 3 ==================== #
# ================================================= #
# * Simulate x ~ N(mu = 0.5, sd = 0.4)
# * Estimate parameters using Maximum Likelihood
# * Use normal distribution for the estimation,
#   in accord with x being normally distributed.
# ===== Model parameters ======== #
n    <- 500
mean <- 0.5
sd   <- 0.4
mu_start <- 0
sd_start <- 1
# ===== Simulation of x  ======== #
x <- rnorm(n = n, mean = mean, sd = sd)
# ===== Likelihood function ===== #
llf <- function(theta, y){
mu <- theta[1]
sd <- theta[2]
n  <- length(y)
cons <- log(2 * pi * sd ** 2)
summ <- sum(((x - mu) / sd) ** 2)
llog <- - 0.5 * (cons + summ / n)
return(-llog)
}
# ===== Maximisation problem ==== #
# Initial parameters
theta <- c(mu_hat = mu_start,
sd_hat = sd_start)
# Maximisation
fit <- optim(theta, llf, y = x, hessian = TRUE)
# Finding std errors and interval for parameters
fisher_info <- solve(fit$hessian)
std_errors  <- sqrt(diag(fisher_info))
upper       <- fit$par + 1.96 * std_errors
lower       <- fit$par - 1.96 * std_errors
interval    <- data.frame(value = fit$par,
upper = upper,
lower = lower)
interval
# ================================================= #
#' Conclusion:
#' Both parameters seem to be found correctly.
#'
#' The interval for the parameters is unacceptably
#' wide. This begs the question of whether I have
#' found the std errors correctly.
#'
#' Finally, changing simulations from 500 to
#' 5,000,000 does not seem to change the size of
#' the interval. Computing time is substantially
#' increased.
# ================================================= #
# Second, we analyse their autocorrelation
acf_ret <- rep(0, 20)
for (i in 1:length(acf_ret)){
acf_ret[i] <- acf(spy_log_ret, i)
}
acf_ret
acf_ret2 <- rep(0, 20)
for (i in 1:length(acf_ret2)){
acf_ret2[i] <- acf(slr2, i)
}
acf_ret2
# Our interest is in the SP500 index
getSymbols("SPY",
from = '2010-01-02',
to = '2015-09-17',
src = 'yahoo',
adjust = TRUE)
# Quantmod provides a way to "getSymbols"
library(quantmod)
# gridExtra provides an easy way to illustrate multiple plots
library(gridExtra)
library(ggplot2)
# source contains function for auto correlation analysis
source('C:/Users/wigr11ab/Dropbox/KU/K3/FE/Code/acf_function.R')
# ================================== #
# ========= Collecting data ======== #
# ================================== #
# Our interest is in the SP500 index
getSymbols("SPY",
from = '2010-01-02',
to = '2015-09-17',
src = 'yahoo',
adjust = TRUE)
head(spy)
# ================================================= #
# =========== Pricing a compound option =========== #
# ================================================= #
source("C:/Users/wigr11ab/Dropbox/KU/K3/MCM/bs_function.R")
# Simulate n paths for T1
# Simulate k paths for T2 for each
# stock path simulated at T1
# We need only the final period stock price
s    <- 100
n    <- 10
m    <- 100000
r    <- 0.05
v    <- 0.2
k_1  <- 10
k_2  <- 100
mat1 <- 2
mat2 <- 3
compound_mc <- function(sim1, sim2, s, r, v, k_1, k_2, mat1, mat2){
# sim1 = n
# sim2 = m
# Discount factors
dc_1  <- exp(-r * mat1)
dc_2  <- exp(-r *  (mat2 - mat1))
# Price matrices
s_1 <- rep(s, sim1)
s_2 <- matrix(0,   nrow = sim1, ncol = sim2)
# Random numbers
z   <- rnorm(n = sim1, 0, 1)
z_2 <- rnorm(n = sim1 * sim2, 0, 1)
z_2 <- matrix(z_2, nrow = sim1, ncol = sim2)
# First, compute T1 stock prices for n stocks
s_1 <- s_1 * exp((r - 0.5 * v ** 2) * mat1 + v * sqrt(mat1) * z)
# For each n stock price at T1, m stock prices are computed for T2
for (i in 1:sim1){
s_2[i,] <- s_1[i] * exp((r - 0.5 * v ** 2) *  (mat2 - mat1) + v * sqrt (mat2 - mat1) * z_2[i,])
}
# The second option price is the T1-PV of the average of each of m paths
c_2 <- 1 / sim2 * dc_2 * rowSums(pmax(s_2 - k_2, 0))
# The final option price is the T0-PV of the average of each of n paths
c_1 <- dc_1 * pmax(c_2 - k_1, 0)
C   <- sum(c_1) / sim1
se  <- sqrt(1 / (sim1 - 1) * sum((c_1 - C) ** 2))
se  <- se / sqrt(sim1)
lower = round(C - 1.96 * se, 4)
upper = round(C + 1.96 * se, 4)
int <- c(lower = lower,
estimate = round(C,4),
upper = upper,
se = round(se, 4))
# ================================================= #
# Applying BS-pricing formula to compare with bias.
# ================================================= #
C_2 <- bs(price = s_1, vol = v, mat = mat2 - mat1, rate = r, strike = k_2)
# At time 0, C_2 was derived analytically and must
# be discounted to time 0.
C_2 <- exp(-r * 1) * sum(C_2) / n
# At time 0, we have C_2 as input prices to derive
# the price of the compound option.
C_1 <- bs(price = C_2, vol = v, mat = mat1, rate = r, strike = k_1)
abs_bias <- round(C - C_1, 4)
rel_bias <- round((C - C_1) / C_1 * 100, 4)
return(list(MC_price = int,
BS_price = round(C_1,4),
abs_bias = abs_bias,
rel_bias = rel_bias))
}
prices <- compound_mc(n, m, s, r, v, k_1, k_2, mat1, mat2)
#' Conclusion: We have now priced a compound option.
#' The compound option is biased high in this example.
#' Using the BS-formula to derive analytical solutions,
#' we confirm the MC method is bias high.
paste0("MC Standard error is: ", prices$MC_price[4], ".", sep = "")
paste0("Black Scholes price is: ", prices$BS_price, " USD.", sep = "")
paste0("MC price is: ", prices$MC_price[2], " USD.", sep = "")
paste0("Surcharge of: ", prices$abs_bias, " USD on actual price.", sep = "")
# Or in percent
paste0("Surcharge of: ", prices$rel_bias, " percent of actual price.", sep = "")
# ================================================= #
# Quantmod provides a way to "getSymbols"
library(quantmod)
# gridExtra provides an easy way to illustrate multiple plots
library(gridExtra)
library(ggplot2)
# source contains function for auto correlation analysis
source('C:/Users/wigr11ab/Dropbox/KU/K3/FE/Code/acf_function.R')
# ================================== #
# ========= Collecting data ======== #
# ================================== #
# Our interest is in the SP500 index
getSymbols("SPY",
from = '2010-01-02',
to = '2015-09-17',
src = 'yahoo',
adjust = TRUE)
spy <- SPY$SPY.Adjusted
# Quickly see what it looks like
ggplot(NULL, aes(x = index(spy),
y = spy,
col = 'red')) +
geom_line() +
ylab("Price (USD)") +
xlab(NULL) +
guides(col = FALSE)
spy_log_ret <- log(spy) - log(lag(spy))
spy_log_ret <- spy_log_ret[-1]
slr2 <- spy_log_ret ** 2
# Second, we analyse their autocorrelation
acf_ret <- rep(0, 20)
for (i in 1:length(acf_ret)){
acf_ret[i] <- acf(spy_log_ret, i)
}
acf_ret
spy_log_ret
x <- spy_log_ret
# Initial parameters
periods     <- length(x)
head(x)
x_bar       <- mean(x)
x_bar
plot(spy)
plot(spy_log_ret)
# Intermediate computations
diff        <- x - x_bar
diff
head(diff)
diff_sqr    <- diff ** 2
diff_lag    <- lag(x = x, n = lags) - x_bar
lags <- 1
diff_lag    <- lag(x = x, n = lags) - x_bar
# Subsetting to remove NA
diff        <- diff[-c(1:lags)]
diff
head(diff)
# Intermediate computations
diff        <- x - x_bar
head(diff)
diff_sqr    <- diff ** 2
diff_lag    <- lag(x = x, n = lags) - x_bar
head(diff_sqrt)
head(diff_sqr)
head(diff_lag)
lags <- 2
# Intermediate computations
diff        <- x - x_bar
diff_sqr    <- diff ** 2
diff_lag    <- lag(x = x, n = lags) - x_bar
# Subsetting to remove NA
diff        <- diff[-c(1:lags)]
diff_sqr    <- diff_sqr[-c(1:lags)]
diff_lag    <- diff_lag[-c(1:lags)]
head(diff)
head(diff_lag)
# Final computation
acf         <- sum(diff * diff_lag) / sum(diff_sqr)
acf
# Initial parameters
x_bar       <- mean(x[-c(1:lags)])
# Intermediate computations
diff        <- x - x_bar
diff_sqr    <- diff ** 2
diff_lag    <- lag(x = x, n = lags) - x_bar
# Subsetting to remove NA
diff        <- diff[-c(1:lags)]
diff_sqr    <- diff_sqr[-c(1:lags)]
diff_lag    <- diff_lag[-c(1:lags)]
# Final computation
acf         <- sum(diff * diff_lag) / sum(diff_sqr)
acf
spy_log_ret <- log(spy) - log(lag(spy))
spy_log_ret <- spy_log_ret[-1]
slr2 <- spy_log_ret ** 2
# Second, we analyse their autocorrelation
acf_ret <- rep(0, 20)
for (i in 1:length(acf_ret)){
acf_ret[i] <- acf(spy_log_ret, i)
}
acf_ret2 <- rep(0, 20)
for (i in 1:length(acf_ret2)){
acf_ret2[i] <- acf(slr2, i)
}
warnings()
acf(spy_log_ret, 2)
# Quantmod provides a way to "getSymbols"
library(quantmod)
# gridExtra provides an easy way to illustrate multiple plots
library(gridExtra)
library(ggplot2)
# source contains function for auto correlation analysis
source('C:/Users/wigr11ab/Dropbox/KU/K3/FE/Code/acf_function.R')
# ================================== #
# ========= Collecting data ======== #
# ================================== #
# Our interest is in the SP500 index
getSymbols("SPY",
from = '2010-01-02',
to = '2015-09-17',
src = 'yahoo',
adjust = TRUE)
spy <- SPY$SPY.Adjusted
# Quickly see what it looks like
ggplot(NULL, aes(x = index(spy),
y = spy,
col = 'red')) +
geom_line() +
ylab("Price (USD)") +
xlab(NULL) +
guides(col = FALSE)
# =================================== #
# ======== Analysing returns ======== #
# =================================== #
# First we transform price data into log returns (and squared)
spy_log_ret <- log(spy) - log(lag(spy))
spy_log_ret <- spy_log_ret[-1]
slr2 <- spy_log_ret ** 2
# Second, we analyse their autocorrelation
acf_ret <- rep(0, 20)
for (i in 1:length(acf_ret)){
acf_ret[i] <- acf_fct(spy_log_ret, i)
}
acf_ret2 <- rep(0, 20)
for (i in 1:length(acf_ret2)){
acf_ret2[i] <- acf_fct(slr2, i)
}
plot(acf_ret)
acf(spy_log_ret, 20)
head(lag(x = x, n = lags))
head(x)
plot(x)
plot(x**2)
head(diff * diff_sqr)
head(diff_lag)
sum(diff_lag)
sum(diff * diff_sqr)
sum(diff * diff_sqr) / sum(diff_lag)
acf_fct(spy_log_ret, 2)
sum(diff * diff_lag) / sum(diff_sqr)
acf_fct(spy_log_ret, 5)
cor(x, lag(x, 1))
cor(x, lag(x = x, n = 1))
cor(x[-1], lag(x = x, n = 1)[-1])
var(x[-1] ** 2)
cor(x[-1], lag(x = x, n = 1)[-1]) / var(x[-1] ** 2)
cor(x[-1], lag(x = x, n = 1)[-1]) / var(x ** 2)
cor(x[-1], lag(x = x, n = 1)[-1]) / var(x[-1])
